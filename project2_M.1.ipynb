{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepw98/project2/blob/main/project2_M.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xtpStzXks1l",
        "outputId": "5572adcf-191d-45af-e0a3-c37dbd3497e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/fire_detection_few_shot /content/fire_detection_few_shot"
      ],
      "metadata": {
        "id": "OuM8U8pCk9c3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "RPnj2FZkLUvT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotFireDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, augment_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Path to the dataset folder.\n",
        "            transform (torchvision.transforms): Transformations for validation/testing.\n",
        "            augment_transform (torchvision.transforms): Transformations for training (augmentation).\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.augment_transform = augment_transform\n",
        "        self.class_to_images = self._load_images_by_class()\n",
        "\n",
        "    def _load_images_by_class(self):\n",
        "        class_to_images = {}\n",
        "        for class_name in os.listdir(self.data_dir):\n",
        "            class_path = os.path.join(self.data_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                class_to_images[class_name] = [\n",
        "                    os.path.join(class_path, img) for img in os.listdir(class_path)\n",
        "                ]\n",
        "        return class_to_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_to_images.keys())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        class_name = list(self.class_to_images.keys())[index]\n",
        "        images = self.class_to_images[class_name]\n",
        "\n",
        "        # Apply augmentations to the images if augmentation transforms are provided\n",
        "        transformed_images = [\n",
        "            self.augment_transform(Image.open(img).convert(\"RGB\")) if self.augment_transform else self.transform(Image.open(img).convert(\"RGB\"))\n",
        "            for img in images\n",
        "        ]\n",
        "\n",
        "        return class_name, transformed_images\n"
      ],
      "metadata": {
        "id": "9zXAETqXLwxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match model input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # EfficientNet normalization\n",
        "])\n",
        "\n",
        "augment_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    # Other existing transformations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "XKaH23usB_z3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    RandomHorizontalFlip,\n",
        "    RandomRotation,\n",
        "    ColorJitter,\n",
        "    RandomResizedCrop,\n",
        "    GaussianBlur,\n",
        "    RandomErasing,\n",
        "    ToTensor,\n",
        "    Normalize,\n",
        "    Compose\n",
        ")\n",
        "\n",
        "augment_transform = Compose([\n",
        "    RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with scale adjustments\n",
        "    RandomHorizontalFlip(p=0.5),               # Horizontal flip\n",
        "    RandomRotation(15),                        # Rotate by Â±15 degrees\n",
        "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # Adjust color\n",
        "    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),  # Apply Gaussian blur\n",
        "    RandomErasing(p=0.3, scale=(0.02, 0.2)),   # Random erasing\n",
        "    ToTensor(),                                # Convert to tensor\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n"
      ],
      "metadata": {
        "id": "b1rQoQyFFjYT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize training dataset with augmentation\n",
        "train_dataset = FewShotFireDataset(\n",
        "    data_dir='/content/fire_detection_few_shot/train',\n",
        "    augment_transform=augment_transform  # For training\n",
        ")\n",
        "\n",
        "# Initialize testing dataset without augmentation\n",
        "test_dataset = FewShotFireDataset(\n",
        "    data_dir='/content/fire_detection_few_shot/test',\n",
        "    transform=transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "4u9F2TrYCE1_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotSampler:\n",
        "    def __init__(self, dataset, n_way=2, k_shot=5, q_query=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset (FewShotFireDataset): The dataset to sample from.\n",
        "            n_way (int): Number of classes per episode.\n",
        "            k_shot (int): Number of support samples per class.\n",
        "            q_query (int): Number of query samples per class.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_query = q_query\n",
        "\n",
        "    def sample_episode(self):\n",
        "      # Convert dict_keys to list to ensure compatibility with random.sample\n",
        "      class_list = list(self.dataset.class_to_images.keys())\n",
        "\n",
        "      # Randomly select N classes\n",
        "      selected_classes = random.sample(class_list, self.n_way)\n",
        "\n",
        "      support_images, support_labels, query_images, query_labels = [], [], [], []\n",
        "\n",
        "      # Create support and query sets\n",
        "      label_map = {class_name: i for i, class_name in enumerate(selected_classes)}\n",
        "      for class_name in selected_classes:\n",
        "          images = self.dataset.class_to_images[class_name]\n",
        "          sampled_images = random.sample(images, self.k_shot + self.q_query)\n",
        "\n",
        "          support_images += sampled_images[:self.k_shot]\n",
        "          query_images += sampled_images[self.k_shot:]\n",
        "\n",
        "          # Labels\n",
        "          support_labels += [label_map[class_name]] * self.k_shot\n",
        "          query_labels += [label_map[class_name]] * self.q_query\n",
        "\n",
        "      return support_images, support_labels, query_images, query_labels\n"
      ],
      "metadata": {
        "id": "IdBxEit-MBKr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match model input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # EfficientNet normalization\n",
        "])\n"
      ],
      "metadata": {
        "id": "FaZxnNA8MGCh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to preprocess support and query sets in each batch.\n",
        "    \"\"\"\n",
        "    support_images, support_labels, query_images, query_labels = [], [], [], []\n",
        "    for support, support_lbl, query, query_lbl in batch:\n",
        "        support_images += support\n",
        "        support_labels += support_lbl\n",
        "        query_images += query\n",
        "        query_labels += query_lbl\n",
        "\n",
        "    # Apply transforms and convert to tensors\n",
        "    support_images = torch.stack([transform(Image.open(img).convert(\"RGB\")) for img in support_images])\n",
        "    query_images = torch.stack([transform(Image.open(img).convert(\"RGB\")) for img in query_images])\n",
        "    support_labels = torch.tensor(support_labels)\n",
        "    query_labels = torch.tensor(query_labels)\n",
        "\n",
        "    return support_images, support_labels, query_images, query_labels\n",
        "\n",
        "# Initialize Dataset and Sampler\n",
        "dataset = FewShotFireDataset(data_dir='/content/fire_detection_few_shot/train', transform=transform)\n",
        "sampler = FewShotSampler(dataset, n_way=2, k_shot=5, q_query=5)\n",
        "\n",
        "# Wrap the sampler into a DataLoader\n",
        "def episodic_loader(sampler, num_episodes):\n",
        "    for _ in range(num_episodes):\n",
        "        yield sampler.sample_episode()\n",
        "\n",
        "dataloader = DataLoader(episodic_loader(sampler, num_episodes=100), batch_size=1, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "zRjs4YuQMKPu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodicDataset:\n",
        "    def __init__(self, sampler, num_episodes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sampler (FewShotSampler): The sampler that generates episodes.\n",
        "            num_episodes (int): Number of episodes for training or testing.\n",
        "        \"\"\"\n",
        "        self.sampler = sampler\n",
        "        self.num_episodes = num_episodes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_episodes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generates one episode using the sampler.\n",
        "        \"\"\"\n",
        "        return self.sampler.sample_episode()\n"
      ],
      "metadata": {
        "id": "Txy_gsd6MqbW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EM3QT3HFCfcB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Dataset and Sampler\n",
        "episodic_dataset = EpisodicDataset(sampler, num_episodes=100)\n",
        "\n",
        "# Wrap into a DataLoader\n",
        "dataloader = DataLoader(episodic_dataset, batch_size=1, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "HmJQcvMrMsZq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Dataset and Sampler\n",
        "train_sampler = FewShotSampler(train_dataset, n_way=2, k_shot=5, q_query=5)\n",
        "test_sampler = FewShotSampler(test_dataset, n_way=2, k_shot=5, q_query=5)\n",
        "\n",
        "# Wrap into DataLoaders\n",
        "train_loader = DataLoader(EpisodicDataset(train_sampler, num_episodes=100), batch_size=1, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(EpisodicDataset(test_sampler, num_episodes=10), batch_size=1, collate_fn=collate_fn)\n",
        "\n",
        "# Training loop\n",
        "for support_images, support_labels, query_images, query_labels in train_loader:\n",
        "    print(\"Support Images Shape:\", support_images.shape)\n",
        "    print(\"Query Images Shape:\", query_images.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3UoGn8lCiqT",
        "outputId": "2b8d4592-a708-4ceb-8f5e-65e7b8ea1649"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Images Shape: torch.Size([10, 3, 224, 224])\n",
            "Query Images Shape: torch.Size([10, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for support_images, support_labels, query_images, query_labels in dataloader:\n",
        "    print(\"Support Images Shape:\", support_images.shape)\n",
        "    print(\"Support Labels Shape:\", support_labels.shape)\n",
        "    print(\"Query Images Shape:\", query_images.shape)\n",
        "    print(\"Query Labels Shape:\", query_labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fkyjq3qMxsM",
        "outputId": "e6f52e91-a2a2-48e5-e38c-fc5f0323db8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Images Shape: torch.Size([10, 3, 224, 224])\n",
            "Support Labels Shape: torch.Size([10])\n",
            "Query Images Shape: torch.Size([10, 3, 224, 224])\n",
            "Query Labels Shape: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import efficientnet_b0\n"
      ],
      "metadata": {
        "id": "O72-OIXLQEA8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingNetwork, self).__init__()\n",
        "        # Load pre-trained EfficientNet-B0 and remove the classifier\n",
        "        self.base_model = efficientnet_b0(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.base_model.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features and flatten\n",
        "        x = self.feature_extractor(x).squeeze(-1).squeeze(-1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AzPv_TDsQD5q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class MatchingNetworkWithPretrainedEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_dim, n_way, k_shot):\n",
        "        super(MatchingNetworkWithPretrainedEmbedding, self).__init__()\n",
        "        # Use a pretrained ResNet backbone as the embedding network\n",
        "        pretrained_resnet = models.resnet50(pretrained=True)  # Load ResNet50 with pretrained weights\n",
        "\n",
        "\n",
        "        self.embedding_net = nn.Sequential(*list(pretrained_resnet.children())[:-1])  # Remove FC layer\n",
        "\n",
        "        # Linear layer to project to the desired embedding dimension\n",
        "        self.projector = nn.Linear(pretrained_resnet.fc.in_features, embedding_dim)\n",
        "\n",
        "        # Softmax over cosine similarities\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images):\n",
        "        # Extract embeddings for support and query images\n",
        "        support_embeddings = self.embedding_net(support_images).squeeze()\n",
        "        query_embeddings = self.embedding_net(query_images).squeeze()\n",
        "\n",
        "        # Project embeddings to desired dimension\n",
        "        support_embeddings = self.projector(support_embeddings)\n",
        "        query_embeddings = self.projector(query_embeddings)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        support_embeddings = nn.functional.normalize(support_embeddings, p=2, dim=1)\n",
        "        query_embeddings = nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
        "\n",
        "        # Calculate cosine similarity between support and query embeddings\n",
        "        similarities = torch.matmul(query_embeddings, support_embeddings.T)\n",
        "\n",
        "        # Softmax over similarities\n",
        "        attention_weights = self.softmax(similarities)\n",
        "\n",
        "        # Aggregate support labels weighted by attention\n",
        "        support_labels_one_hot = nn.functional.one_hot(support_labels, num_classes=n_way).float()\n",
        "        query_predictions = torch.matmul(attention_weights, support_labels_one_hot)\n",
        "\n",
        "        return query_predictions\n"
      ],
      "metadata": {
        "id": "Xze8rKV6PWtM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUhlXkFUUi0N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize Matching Network\n",
        "# embedding_dim = 1280  # EfficientNet-B0 output feature dimension\n",
        "# n_way = 2\n",
        "# k_shot = 5\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = MatchingNetwork(embedding_dim, n_way, k_shot).to(device)\n",
        "embedding_dim = 128  # Define your desired embedding dimension\n",
        "n_way = 2            # Number of classes\n",
        "k_shot = 5           # Number of examples per class in support set\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MatchingNetworkWithPretrainedEmbedding(embedding_dim, n_way, k_shot).to(device)\n",
        "for param in model.embedding_net.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (support_images, support_labels, query_images, query_labels) in enumerate(dataloader):\n",
        "        # Move data to GPU\n",
        "        support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
        "        query_images, query_labels = query_images.to(device), query_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        class_probabilities = model(support_images, support_labels, query_images)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(class_probabilities, query_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3F8jiPMQlxA",
        "outputId": "6fa731dd-6eb1-4a1d-f429-e70124670f8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|ââââââââââ| 97.8M/97.8M [00:00<00:00, 167MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4920\n",
            "Epoch [2/20], Loss: 0.4224\n",
            "Epoch [3/20], Loss: 0.4057\n",
            "Epoch [4/20], Loss: 0.3994\n",
            "Epoch [5/20], Loss: 0.3958\n",
            "Epoch [6/20], Loss: 0.3927\n",
            "Epoch [7/20], Loss: 0.3943\n",
            "Epoch [8/20], Loss: 0.3910\n",
            "Epoch [9/20], Loss: 0.3893\n",
            "Epoch [10/20], Loss: 0.3877\n",
            "Epoch [11/20], Loss: 0.3906\n",
            "Epoch [12/20], Loss: 0.3883\n",
            "Epoch [13/20], Loss: 0.3879\n",
            "Epoch [14/20], Loss: 0.3875\n",
            "Epoch [15/20], Loss: 0.3898\n",
            "Epoch [16/20], Loss: 0.3864\n",
            "Epoch [17/20], Loss: 0.3875\n",
            "Epoch [18/20], Loss: 0.3875\n",
            "Epoch [19/20], Loss: 0.3881\n",
            "Epoch [20/20], Loss: 0.3860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for support_images, support_labels, query_images, query_labels in dataloader:\n",
        "        # Move data to GPU\n",
        "        support_images, support_labels = support_images.cuda(), support_labels.cuda()\n",
        "        query_images, query_labels = query_images.cuda(), query_labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        class_probabilities = model(support_images, support_labels, query_images)\n",
        "\n",
        "        # Predicted classes\n",
        "        predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct += (predicted_classes == query_labels).sum().item()\n",
        "        total += query_labels.size(0)\n",
        "\n",
        "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOPDF729Q98I",
        "outputId": "83ff59d0-70a7-4719-87c6-e6975759fb93"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "\n",
        "# Initialize variables to store ground truth and predictions\n",
        "all_query_labels = []\n",
        "all_predicted_classes = []\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for support_images, support_labels, query_images, query_labels in dataloader:\n",
        "        # Move data to GPU\n",
        "        support_images, support_labels = support_images.cuda(), support_labels.cuda()\n",
        "        query_images, query_labels = query_images.cuda(), query_labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        class_probabilities = model(support_images, support_labels, query_images)\n",
        "\n",
        "        # Predicted classes\n",
        "        predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
        "\n",
        "        # Collect predictions and labels for metric computation\n",
        "        all_query_labels.extend(query_labels.cpu().numpy())\n",
        "        all_predicted_classes.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct += (predicted_classes == query_labels).sum().item()\n",
        "        total += query_labels.size(0)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Compute precision, recall, F1 score, and confusion matrix\n",
        "precision = precision_score(all_query_labels, all_predicted_classes, average=\"weighted\")\n",
        "recall = recall_score(all_query_labels, all_predicted_classes, average=\"weighted\")\n",
        "f1 = f1_score(all_query_labels, all_predicted_classes, average=\"weighted\")\n",
        "conf_matrix = confusion_matrix(all_query_labels, all_predicted_classes)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Dz2r9r44qK",
        "outputId": "39a5c408-007b-4dc2-d3ee-c8533a9256c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Confusion Matrix:\n",
            "[[500   0]\n",
            " [  0 500]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "episodes = 100  # Number of evaluation episodes\n",
        "\n",
        "for _ in range(episodes):\n",
        "    support_images, support_labels, query_images, query_labels = next(iter(dataloader))\n",
        "    support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
        "    query_images, query_labels = query_images.to(device), query_labels.to(device)\n",
        "\n",
        "    class_probabilities = model(support_images, support_labels, query_images)\n",
        "    predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
        "\n",
        "    correct += (predicted_classes == query_labels).sum().item()\n",
        "    total += query_labels.size(0)\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Average Accuracy over {episodes} episodes: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJGGLQzmWO73",
        "outputId": "12b5548a-9145-4271-e6e0-2ea514ebb936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy over 100 episodes: 99.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_dataset(data_dir):\n",
        "    \"\"\"\n",
        "    Prepare image data and labels from a directory.\n",
        "    Args:\n",
        "        data_dir (str): Path to the dataset directory.\n",
        "    Returns:\n",
        "        data (list): List of image file paths.\n",
        "        labels (list): Corresponding class labels.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    class_to_idx = {}\n",
        "\n",
        "    # Assign an integer index to each class\n",
        "    for idx, class_name in enumerate(sorted(os.listdir(data_dir))):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_to_idx[class_name] = idx\n",
        "\n",
        "            # Collect image file paths and labels\n",
        "            for img_name in os.listdir(class_path):\n",
        "                img_path = os.path.join(class_path, img_name)\n",
        "                if img_name.endswith(('.png', '.jpg', '.jpeg')):  # Filter image files\n",
        "                    data.append(img_path)\n",
        "                    labels.append(idx)\n",
        "\n",
        "    return data, labels, class_to_idx\n",
        "\n",
        "# Example Usage\n",
        "data_dir = \"/content/fire_detection_few_shot/train\"  # Path to the root dataset directory\n",
        "data1, labels1, class_to_idx = prepare_dataset(data_dir)\n",
        "\n",
        "# Output some stats\n",
        "print(f\"Total images: {len(data1)}\")\n",
        "print(f\"Class-to-index mapping: {class_to_idx}\")\n",
        "\n",
        "# Example: Creating a CustomDataset instance\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KN7oJImiNRum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f824f59-8cf8-4261-ba29-d8433dae111c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 294\n",
            "Class-to-index mapping: {'Fire': 0, 'No_Fire': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Custom Dataset for data and labels\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list): List of image file paths or tensors.\n",
        "            labels (list): Corresponding labels.\n",
        "            transform (callable, optional): Transform to apply to the data.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "\n",
        "        # If `img` is a file path, open it as an image\n",
        "        if isinstance(img, str):  # Handle file paths\n",
        "            img = Image.open(img).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "# Example: Assuming you have `data` and `labels`\n",
        "# Example data (replace with actual\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = CustomDataset(data1, labels1, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Iterate through DataLoader\n",
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx+1}:\")\n",
        "    print(f\"Images shape: {images.shape}\")\n",
        "    print(f\"Labels: {labels}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvE6eSkf-cgn",
        "outputId": "86f4aae9-5ee2-47ce-8d98-e4577889558d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 1, 1, 1, 1, 1])\n",
            "Batch 2:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 1, 1, 1, 1])\n",
            "Batch 3:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 0, 1, 0, 1, 0, 0, 1])\n",
            "Batch 4:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 1, 0, 0, 1, 0])\n",
            "Batch 5:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 0, 0, 1, 1])\n",
            "Batch 6:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 1, 1, 0, 0, 1])\n",
            "Batch 7:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 0, 1, 0, 0, 1, 0])\n",
            "Batch 8:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 1, 1, 1, 0])\n",
            "Batch 9:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 1, 0, 1, 0, 0, 1])\n",
            "Batch 10:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 1, 1, 0, 0, 1, 0])\n",
            "Batch 11:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 1, 1, 0, 1, 0])\n",
            "Batch 12:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 1, 1, 1, 1, 0, 0])\n",
            "Batch 13:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 0, 1, 1, 1, 1, 0])\n",
            "Batch 14:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 1, 1, 0, 0, 1, 0])\n",
            "Batch 15:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 0, 1, 1, 1, 0, 1, 1])\n",
            "Batch 16:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 0, 1, 0, 0, 1, 1])\n",
            "Batch 17:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 0, 0, 1, 0, 0])\n",
            "Batch 18:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 1, 1, 1, 0, 1])\n",
            "Batch 19:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 0, 0, 1, 0, 0])\n",
            "Batch 20:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 1, 1, 1, 1, 0, 0])\n",
            "Batch 21:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 1, 1, 1, 1, 1])\n",
            "Batch 22:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 1, 1, 1, 0, 1])\n",
            "Batch 23:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 0, 1, 1, 1, 1, 1, 0])\n",
            "Batch 24:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 0, 0, 0, 1, 1])\n",
            "Batch 25:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 1, 1, 0, 0, 0, 1])\n",
            "Batch 26:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 1, 0, 0, 0, 0, 1])\n",
            "Batch 27:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "Batch 28:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 0, 0, 1, 1, 0, 1])\n",
            "Batch 29:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 1, 0, 1, 1, 1, 0])\n",
            "Batch 30:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Batch 31:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 1, 1, 0, 0])\n",
            "Batch 32:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 0, 0, 0, 0, 0, 0, 1])\n",
            "Batch 33:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "Batch 34:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([0, 1, 1, 0, 1, 0, 1, 1])\n",
            "Batch 35:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 0, 1, 0, 0, 0, 1, 1])\n",
            "Batch 36:\n",
            "Images shape: torch.Size([8, 3, 224, 224])\n",
            "Labels: tensor([1, 1, 0, 0, 0, 1, 0, 0])\n",
            "Batch 37:\n",
            "Images shape: torch.Size([6, 3, 224, 224])\n",
            "Labels: tensor([0, 0, 0, 0, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "\n",
        "# ResNet Backbone\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        resnet = resnet50(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove final classification layer\n",
        "        self.fc = nn.Linear(2048, 256)  # Project to 256-dim space\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten features\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Triplet Loss\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_dist = torch.nn.functional.pairwise_distance(anchor, positive, p=2)\n",
        "        neg_dist = torch.nn.functional.pairwise_distance(anchor, negative, p=2)\n",
        "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0).mean()\n",
        "        return loss\n",
        "\n",
        "# Hard Triplet Mining\n",
        "def hard_triplet_mining(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Perform hard triplet mining within a batch.\n",
        "    Args:\n",
        "        embeddings (torch.Tensor): Embeddings of the batch (shape: [batch_size, embedding_dim]).\n",
        "        labels (torch.Tensor): Corresponding labels of the batch (shape: [batch_size]).\n",
        "    Returns:\n",
        "        anchors, positives, negatives: Hard triplets selected from the batch.\n",
        "    \"\"\"\n",
        "    batch_size = embeddings.size(0)\n",
        "    distance_matrix = torch.cdist(embeddings, embeddings, p=2)  # Pairwise distances\n",
        "\n",
        "    anchors, positives, negatives = [], [], []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        positive_mask = labels == labels[i]\n",
        "        negative_mask = labels != labels[i]\n",
        "\n",
        "        # Hardest positive: max distance among positives\n",
        "        hardest_positive = torch.argmax(distance_matrix[i][positive_mask]).item()\n",
        "        # Hardest negative: min distance among negatives\n",
        "        hardest_negative = torch.argmin(distance_matrix[i][negative_mask]).item()\n",
        "\n",
        "        anchors.append(embeddings[i])\n",
        "        positives.append(embeddings[hardest_positive])\n",
        "        negatives.append(embeddings[hardest_negative])\n",
        "\n",
        "    return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)\n",
        "\n",
        "# Training Loop\n",
        "def train_with_triplet_loss(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for data1, labels1 in dataloader:\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            embeddings = model(data)\n",
        "\n",
        "            # Hard triplet mining\n",
        "            anchors, positives, negatives = hard_triplet_mining(embeddings, labels)\n",
        "\n",
        "            # Compute triplet loss\n",
        "            loss = criterion(anchors, positives, negatives)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# # Example Usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Example dataset\n",
        "#     data = [torch.rand(3, 224, 224) for _ in range(100)]  # Random images\n",
        "#     labels = torch.tensor([random.randint(0, 4) for _ in range(100)])  # Random labels (5 classes)\n",
        "\n",
        "    dataset = CustomDataset(data1, labels1)\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = FeatureExtractor().cuda()\n",
        "    criterion = TripletLoss(margin=1.0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model\n",
        "    train_with_triplet_loss(model, dataloader, criterion, optimizer, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "HKwaVAWvQi5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMZ7zraTQixD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}